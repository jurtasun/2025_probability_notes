\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\babel@aux{greek}{}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Index}{\EnsureStandardFontEncoding {iv}}{}\protected@file@percent }
\citation{ioannidis2005why}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{The purpose of these notes}{1}{}\protected@file@percent }
\citation{spiegelhalter2019art}
\citation{degroot2012probability}
\citation{bandyopadhyay2011philosophy}
\citation{openintro2025}
\citation{pishronik2014introduction}
\citation{finkel2007ancient}
\citation{david1962games}
\citation{cicero45bce}
\citation{cardano1663ludo}
\citation{devlin2008unfinished}
\citation{huygens1657ratiociniis}
\citation{bernoulli1713ars}
\citation{hald1990history}
\citation{kolmogorov1933grundbegriffe}
\@writefile{toc}{\contentsline {section}{A bit of history}{3}{}\protected@file@percent }
\citation{ramsey1926}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Descriptive statistics}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Sampling and data types}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Central tendency and variation}{6}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:histogram1}{{1.1a}{7}{}{}{}}
\newlabel{sub@fig:histogram1}{{a}{7}{}{}{}}
\newlabel{fig:histogram2}{{1.1b}{7}{}{}{}}
\newlabel{sub@fig:histogram2}{{b}{7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Histogram representation of $n = 100$ observations drawn from a Gaussian distribution. The RHS shows a clean Gaussian distribution, symmetric around the central value and with no outlier points, while the LHS shows a skewed version, where some outliers make the distribution deviate from the symmetric case.}}{7}{}\protected@file@percent }
\newlabel{fig:histogram_comparison1}{{1.1}{7}{}{}{}}
\newlabel{eq:sample_mean1}{{1.3}{7}{}{}{}}
\newlabel{eq:sample_mean2}{{1.4}{7}{}{}{}}
\newlabel{eq:population_mean}{{1.5}{7}{}{}{}}
\newlabel{fig:histogram1_bins_small}{{1.2a}{8}{}{}{}}
\newlabel{sub@fig:histogram1_bins_small}{{a}{8}{}{}{}}
\newlabel{fig:histogram2_bins_small}{{1.2b}{8}{}{}{}}
\newlabel{sub@fig:histogram2_bins_small}{{b}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Histogram representation of $n = 100$ observations drawn from a Gaussian distribution. The RHS shows a clean Gaussian distribution, symmetric around the central value and with no outlier points, while the LHS shows a skewed version, where some outliers make the distribution deviate from the symmetric case. Narrower binning leads to higher resolution, but it is more sensitive to outliers.}}{8}{}\protected@file@percent }
\newlabel{fig:histogram_comparison2}{{1.2}{8}{}{}{}}
\newlabel{eq:median}{{1.6}{8}{}{}{}}
\newlabel{fig:histogram1_bins_large}{{1.3a}{9}{}{}{}}
\newlabel{sub@fig:histogram1_bins_large}{{a}{9}{}{}{}}
\newlabel{fig:histogram2_bins_large}{{1.3b}{9}{}{}{}}
\newlabel{sub@fig:histogram2_bins_large}{{b}{9}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Histogram representation of $n = 100$ observations drawn from a Gaussian distribution. The RHS shows a clean Gaussian distribution, symmetric around the central value and with no outlier points, while the LHS shows a skewed version, where some outliers make the distribution deviate from the symmetric case. Thicker binning usually implies smaller resolution, but averages the raw observations and it remains more robust against outliers.}}{9}{}\protected@file@percent }
\newlabel{fig:histogram_comparison3}{{1.3}{9}{}{}{}}
\newlabel{eq:sample_variance}{{1.11}{9}{}{}{}}
\newlabel{eq:population_variance}{{1.12}{9}{}{}{}}
\newlabel{eq:sample_std}{{1.13}{10}{}{}{}}
\newlabel{eq:population_std}{{1.14}{10}{}{}{}}
\newlabel{eq:quantiles}{{1.15}{10}{}{}{}}
\citation{pearson1892}
\citation{tukey1977}
\newlabel{fig:box1}{{1.4a}{11}{}{}{}}
\newlabel{sub@fig:box1}{{a}{11}{}{}{}}
\newlabel{fig:box2}{{1.4b}{11}{}{}{}}
\newlabel{sub@fig:box2}{{b}{11}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Box plots representing $n = 100$ observations drawn from a Gaussian distribution. The central black line shows the mean value, representing the central tendency where the bulk of events lie. The shadowed area highlights the standard deviation, as measure of the variability and spread the observations with respect to the mean}}{11}{}\protected@file@percent }
\newlabel{fig:box_comparison}{{1.4}{11}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Data visualization}{11}{}\protected@file@percent }
\citation{hintze1997}
\newlabel{fig:violin1}{{1.5a}{12}{}{}{}}
\newlabel{sub@fig:violin1}{{a}{12}{}{}{}}
\newlabel{fig:violin2}{{1.5b}{12}{}{}{}}
\newlabel{sub@fig:violin2}{{b}{12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Violin plots representing $n = 100$ observations drawn from a Gaussian distribution. The central black line shows the mean value, representing the central tendency where the bulk of events lie. The shadowed area highlights the standard deviation, as measure of the variability and spread the observations with respect to the mean}}{12}{}\protected@file@percent }
\newlabel{fig:violin_comparison}{{1.5}{12}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Dependency, linearity, correlation}{12}{}\protected@file@percent }
\newlabel{eq:linear}{{1.16}{12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Three sets of observations, with the mean value and standard deviation represented as a violin plot [...]. The red line shows the mean value, representing the central tendency where the bulk of events lie. The shadowed area highlights the standard deviation, as measure of the variability and spread the observations with respect to the mean. In this case the three observation sets, or \textit  {samples}, consist of 100 observations. In the case of $\chi _1$, $\chi _2$, $\chi _3$ [...].}}{13}{}\protected@file@percent }
\newlabel{fig:histogram1}{{1.6}{13}{}{}{}}
\newlabel{eq:quadratic}{{1.17}{13}{}{}{}}
\newlabel{eq:polynomial}{{1.18}{13}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Three sets of observations, with the mean value and standard deviation represented as a box plot [...]. The red line shows the mean value, representing the central value where the bulk of events lie, and the shadowed area the standard deviation, as measure of the variability, or how spread the observations are with respect to the mean. In this case the three observation sets, or \textit  {samples}, consist of 100 observations. In the case of $\chi _1$, $\chi _2$, $\chi _3$ [...].}}{14}{}\protected@file@percent }
\newlabel{fig:histogram1}{{1.7}{14}{}{}{}}
\newlabel{eq:correlation}{{1.19}{14}{}{}{}}
\newlabel{eq:covariance}{{1.20}{14}{}{}{}}
\newlabel{eq:expected_value}{{1.21}{14}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Three sets of observations, with the mean value and standard deviation represented as a violin plot [...]. The red line shows the mean value, representing the central value where the bulk of events lie, and the shadowed area the standard deviation, as measure of the variability, or how spread the observations are with respect to the mean. In this case the three observation sets, or \textit  {samples}, consist of 100 observations. In the case of $\chi _1$, $\chi _2$, $\chi _3$ [...].}}{15}{}\protected@file@percent }
\newlabel{fig:histogram1}{{1.8}{15}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Scatter data following linear dependency. The linear function $y(x) = a x + b$ is displayed overlaying the sampling points, with parameters fitted from data.}}{16}{}\protected@file@percent }
\newlabel{fig:linear1}{{1.9}{16}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Scatter data following quadratic dependency. The quadratic function $y(x) = a x + b$ is displayed overlaying the sampling points, with parameters fitted from data.}}{16}{}\protected@file@percent }
\newlabel{fig:quadratic1}{{1.10}{16}{}{}{}}
\citation{anscombe1973graphs}
\citation{spearman1904}
\newlabel{eq:linear_error}{{1.24}{17}{}{}{}}
\newlabel{eq:spearman}{{1.25}{17}{}{}{}}
\citation{kolmogorov1933}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Probability and random events}{21}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What is probability?}{21}{}\protected@file@percent }
\citation{lebesgue1902}
\citation{doob1953}
\citation{billingsley1995}
\newlabel{eq:prob_coin}{{2.1}{22}{}{}{}}
\newlabel{eq:prob_frequentist}{{2.1}{22}{}{}{}}
\newlabel{eq:prob_coin}{{2.2}{22}{}{}{}}
\newlabel{eq:unitarity}{{2.2}{22}{}{}{}}
\citation{vonmises1928}
\citation{bayes1763}
\citation{laplace1812}
\citation{definetti1974}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Discrete random variables}{24}{}\protected@file@percent }
\newlabel{eq:bernoulli1}{{2.6}{24}{}{}{}}
\newlabel{fig:bernoulli1}{{2.1a}{24}{}{}{}}
\newlabel{sub@fig:bernoulli1}{{a}{24}{}{}{}}
\newlabel{fig:bernoulli2}{{2.1b}{24}{}{}{}}
\newlabel{sub@fig:bernoulli2}{{b}{24}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the Bernoulli distribution as a histogram. The horizontal axis displays the random variable $X$, and the height of the bars in the vertical axis the probability for each value of $X$.}}{24}{}\protected@file@percent }
\newlabel{fig:bernoulli_comparison}{{2.1}{24}{}{}{}}
\citation{poisson-judgements}
\newlabel{eq:binomial1}{{2.7}{25}{}{}{}}
\newlabel{eq:poisson1}{{2.8}{25}{}{}{}}
\citation{cardano-ludo}
\newlabel{fig:binomial1}{{2.2a}{26}{}{}{}}
\newlabel{sub@fig:binomial1}{{a}{26}{}{}{}}
\newlabel{fig:binomial1_cum}{{2.2b}{26}{}{}{}}
\newlabel{sub@fig:binomial1_cum}{{b}{26}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Representation of the Binomial distribution as a histogram. The horizontal axis displays the random variable $X$, and the height of the bars in the vertical axis the probability for each value of $X$.}}{26}{}\protected@file@percent }
\newlabel{fig:binomial_comparison}{{2.2}{26}{}{}{}}
\newlabel{eq:discrete_uniform}{{2.10}{26}{}{}{}}
\newlabel{fig:poisson1}{{2.3a}{27}{}{}{}}
\newlabel{sub@fig:poisson1}{{a}{27}{}{}{}}
\newlabel{fig:poisson1_cum}{{2.3b}{27}{}{}{}}
\newlabel{sub@fig:poisson1_cum}{{b}{27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Representation of the Poisson distribution as a histogram. The horizontal axis displays the random variable $X$, and the height of the bars in the vertical axis the probability for each value of $X$.}}{27}{}\protected@file@percent }
\newlabel{fig:poisson_comparison}{{2.3}{27}{}{}{}}
\newlabel{fig:uniform1}{{2.4a}{27}{}{}{}}
\newlabel{sub@fig:uniform1}{{a}{27}{}{}{}}
\newlabel{fig:uniform1_cum}{{2.4b}{27}{}{}{}}
\newlabel{sub@fig:uniform1_cum}{{b}{27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Representation of the discrete Uniform distribution as a histogram. The horizontal axis displays the random variable $X$, and the height of the bars in the vertical axis the probability for each value of $X$.}}{27}{}\protected@file@percent }
\newlabel{fig:uniform_comparison}{{2.4}{27}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Continuous random variables}{27}{}\protected@file@percent }
\citation{laplace-probability}
\newlabel{eq:prob_continous}{{2.11}{28}{}{}{}}
\newlabel{eq:continuous_uniform}{{2.12}{28}{}{}{}}
\newlabel{fig:uniform1}{{2.5a}{29}{}{}{}}
\newlabel{sub@fig:uniform1}{{a}{29}{}{}{}}
\newlabel{fig:uniform2}{{2.5b}{29}{}{}{}}
\newlabel{sub@fig:uniform2}{{b}{29}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Representation of the Uniform distribution as a continuous function. The horizontal axis displays the random variable $x$.}}{29}{}\protected@file@percent }
\newlabel{fig:uniform_comparison}{{2.5}{29}{}{}{}}
\newlabel{eq:exponential}{{2.16}{29}{}{}{}}
\citation{gauss-astronomy}
\newlabel{fig:exponential1}{{2.6a}{30}{}{}{}}
\newlabel{sub@fig:exponential1}{{a}{30}{}{}{}}
\newlabel{fig:exponential2}{{2.6b}{30}{}{}{}}
\newlabel{sub@fig:exponential2}{{b}{30}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Representation of the Exponential distribution as a continuous function. The horizontal axis displays the random variable $x$.}}{30}{}\protected@file@percent }
\newlabel{fig:exponential_comparison}{{2.6}{30}{}{}{}}
\newlabel{eq:gaussian}{{2.25}{30}{}{}{}}
\newlabel{fig:gaussian1}{{2.7a}{31}{}{}{}}
\newlabel{sub@fig:gaussian1}{{a}{31}{}{}{}}
\newlabel{fig:gaussian2}{{2.7b}{31}{}{}{}}
\newlabel{sub@fig:gaussian2}{{b}{31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Representation of the Gaussian distribution as a continuous function. The horizontal axis displays the random variable $x$.}}{31}{}\protected@file@percent }
\newlabel{fig:gaussian_comparison}{{2.7}{31}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Expectation values}{31}{}\protected@file@percent }
\citation{pearson-skewness}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Parameter estimation}{37}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Prediction vs inference}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Parameters, variables, statistics}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The Law of Large Numbers}{39}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Representation of the law or large numbers. The sample mean tends to the population mean as the number of rolls $n$ increases.}}{40}{}\protected@file@percent }
\newlabel{fig:random}{{3.1}{40}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The Central Limit Theorem}{41}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Histogram representing a set of observations and the central limit theorem. The sample mean follows a gaussian distribution as the sample size $n$ increases.}}{41}{}\protected@file@percent }
\newlabel{fig:random}{{3.2}{41}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Histogram representing a set of observations and the central limit theorem. The sample mean follows a gaussian distribution as the sample size $n$ increases.}}{42}{}\protected@file@percent }
\newlabel{fig:random}{{3.3}{42}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Histogram representing a set of observations and the central limit theorem. The sample mean follows a gaussian distribution as the sample size $n$ increases.}}{43}{}\protected@file@percent }
\newlabel{fig:random}{{3.4}{43}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Maximum Likelihood Estimation}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Historical Context:}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Maximum Likelihood?}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of MLE:}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Motivation and intuition}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}The Likelihood and Log-Likelihood functions}{46}{}\protected@file@percent }
\citation{fisher1925}
\citation{fisher1935}
\citation{pearson1900}
\citation{pearson1900}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Introduction to hypothesis testing}{51}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Prediction vs inference}{51}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Representation of the predictive (from theory to experimental verification) and inferential (from data to underlying truth, descriptive and hypothesis testing) approaches to probability and statistics \cite  {pearson1900}.}}{51}{}\protected@file@percent }
\newlabel{fig:prob_vs_stats1}{{4.1}{51}{}{}{}}
\citation{fisher1925}
\citation{pearson1900}
\citation{welch1947}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hypothesis, significance, p-values}{52}{}\protected@file@percent }
\citation{student1908}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Statistical tests: some examples}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Compare sample mean with hypothesized value - One sample t-test}{55}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Representation of the t statistic, following the Student's t distribution, for a particular value of the degrees of freedom ($\nu = 10$). The integral of the shadowed area represents the \textit  {1-sided}, or \textit  {1-tailed} p-value, as the probability of obtaining a result \textit  {at least as extreme} as the one obtained $t_{obs}$.}}{55}{}\protected@file@percent }
\newlabel{fig:t_test1}{{4.2}{55}{}{}{}}
\citation{welch1947}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Compare sample means of two independent groups - Two sample t-test}{57}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Representation of the t statistic, following the Student's t distribution, for a particular value of the degrees of freedom ($\nu = 10$). The integral of the shadowed area represents the \textit  {2-sided}, or \textit  {2-tailed} p-value, as the probability of obtaining a result \textit  {at least as extreme} as the one obtained $t_{obs}$.}}{57}{}\protected@file@percent }
\newlabel{fig:t_test2}{{4.3}{57}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Representation of the Student's t distribution, for different values of the degrees of freedom $\nu $.}}{58}{}\protected@file@percent }
\newlabel{fig:t_distribution}{{4.4}{58}{}{}{}}
\citation{welch1947}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Compare sample variances of two groups - Fisher's exact test}{59}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Representation of the F statistic, following the Fisher distribution, for a particular value of the degrees of freedom ($\nu = 10$). The integral of the shadowed area represents the \textit  {1-sided}, or \textit  {1-tailed} p-value, as the probability of obtaining a result \textit  {at least as extreme} as the one obtained $t_{obs}$.}}{59}{}\protected@file@percent }
\newlabel{fig:f_test1}{{4.5}{59}{}{}{}}
\citation{fisher1925}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Compare variation on more than two groups - Fisher's ANOVA}{61}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Representation of the F statistic, following the Fisher distribution, for a particular value of the degrees of freedom ($\nu = 10$). The integral of the shadowed area represents the \textit  {2-sided}, or \textit  {2-tailed} p-value, as the probability of obtaining a result \textit  {at least as extreme} as the one obtained $t_{obs}$.}}{61}{}\protected@file@percent }
\newlabel{fig:f_test2}{{4.6}{61}{}{}{}}
\citation{pearson1900}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Representation of the Fisher distribution, for different values of the degrees of freedom $\nu _1$, $\nu _2$.}}{63}{}\protected@file@percent }
\newlabel{fig:f_distribution}{{4.7}{63}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Compare distributions and testing for normality - $\chi ^{2}$ test}{63}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Representation of the $chi^{2}$ statistic, following the Pearson $chi^{2}$ distribution, for a particular value of the degrees of freedom ($\nu = 10$). The integral of the shadowed area represents the \textit  {1-sided}, or \textit  {1-tailed} p-value, as the probability of obtaining a result \textit  {at least as extreme} as the one obtained $chi^{2}_{obs}$.}}{64}{}\protected@file@percent }
\newlabel{fig:chi2_test1}{{4.8}{64}{}{}{}}
\citation{student1908}
\citation{fisher1935}
\citation{fisher1925}
\citation{pearson1900}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Representation of the $chi^{2}$ statistic, following the Pearson $chi^{2}$ distribution, for a particular value of the degrees of freedom ($\nu = 10$). The integral of the shadowed area represents the \textit  {2-sided}, or \textit  {2-tailed} p-value, as the probability of obtaining a result \textit  {at least as extreme} as the one obtained $chi^{2}_{obs}$.}}{65}{}\protected@file@percent }
\newlabel{fig:chi2_test2}{{4.9}{65}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Parametric and non-parametric}{65}{}\protected@file@percent }
\citation{welch1947}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Representation of the $chi^{2}$ distribution, for a particular value of the degrees of freedom ($\nu = 10$).}}{66}{}\protected@file@percent }
\newlabel{fig:chi2_distribution}{{4.10}{66}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Comparing data and normalization}{67}{}\protected@file@percent }
\citation{hacking1965logic}
\citation{bayes1763essay}
\citation{laplace1814philosophical}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Introduction to bayesian probability}{71}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Motivation and philosophy}{71}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Foundations of conditional probability}{71}{}\protected@file@percent }
\citation{feller1950probability}
\citation{jaynes2003probability}
\citation{kolmogorov1933foundations}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Bayesian reasoning and applications}{72}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Rigorous mathematical formalism}{72}{}\protected@file@percent }
\citation{lebesgue1902}
\citation{kolmogorov1933}
\citation{doob1953stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Stochasticity and Markov processes}{73}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix: Vectors and matrices: a quick review}{77}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}The roots and rise of algebra}{77}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Vectors and their properties}{79}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Matrices and linear transformations}{80}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Basic algebraic operations}{80}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix: Functions and derivatives: a quick review}{81}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}From curves to calculus: functions}{81}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$.}}{82}{}\protected@file@percent }
\newlabel{fig:functions_point_1}{{B.1}{82}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Change, slope and minima: derivatives}{83}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$, and an increment $\Delta x$ from $x_0$ to $x_0 + \Delta x$. The ratio between the increments in the horizontal axis, $\Delta x$, and the vertical axis $f(x_0 + \Delta x) - f(x_0)$ represents the derivative of the function at that point, that we denote by $f'(x_0)$.}}{83}{}\protected@file@percent }
\newlabel{fig:functions_point_2}{{B.2}{83}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$, and an increment $\Delta x$ from $x_0$ to $x_0 + \Delta x$. The ratio between the increments in the horizontal axis, $\Delta x$, and the vertical axis $f(x_0 + \Delta x) - f(x_0)$ represents the derivative of the function at that point, that we denote by $f'(x_0)$.}}{84}{}\protected@file@percent }
\newlabel{fig:functions_point_2_shadow}{{B.3}{84}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Divergence and gradient: differential calculus}{85}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Appendix: Integrals: a quick review}{87}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$, and an increment $\Delta x$ from $x_0$ to $x_0 + \Delta x$.}}{88}{}\protected@file@percent }
\newlabel{fig:integrals_1}{{C.1}{88}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Indefinite integral as antiderivative}{88}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$, and an increment $\Delta x$ from $x_0$ to $x_0 + \Delta x$. Here we divide the $\Delta x$ increment in smaller steps, aiming to approximate the area under the function. As the subdivisions become smaller, they become a better approximation of the area, and in the limit $\Delta x \rightarrow \infty $ they converge to the Riemann definition.}}{89}{}\protected@file@percent }
\newlabel{fig:integrals_2}{{C.2}{89}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Define definite integral as area under a curve}{89}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.3}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$, and an increment $\Delta x$ from $x_0$ to $x_0 + \Delta x$. Here we divide the $\Delta x$ increment in smaller steps, aiming to approximate the area under the function. As the subdivisions become smaller, they become a better approximation of the area, and in the limit $\Delta x \rightarrow \infty $ they converge to the Riemann definition.}}{90}{}\protected@file@percent }
\newlabel{fig:integrals_2_subdivisions}{{C.3}{90}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}The fundamental theorem of calculus}{90}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.4}{\ignorespaces Representation of a function $f(x)$, a given point $x_0$ and its image $f(x_0)$, and an increment $\Delta x$ from $x_0$ to $x_0 + \Delta x$. Here we divide the $\Delta x$ increment in smaller steps, aiming to approximate the area under the function. As the subdivisions become smaller, they become a better approximation of the area, and in the limit $\Delta x \rightarrow \infty $ they converge to the Riemann definition.}}{91}{}\protected@file@percent }
\newlabel{fig:integrals_3_riemann}{{C.4}{91}{}{}{}}
\bibcite{ioannidis2005why}{1}
\bibcite{spiegelhalter2019art}{2}
\bibcite{degroot2012probability}{3}
\bibcite{bandyopadhyay2011philosophy}{4}
\bibcite{openintro2025}{5}
\bibcite{pishronik2014introduction}{6}
\bibcite{finkel2007ancient}{7}
\bibcite{david1962games}{8}
\bibcite{cicero45bce}{9}
\bibcite{cardano1663ludo}{10}
\bibcite{devlin2008unfinished}{11}
\bibcite{huygens1657ratiociniis}{12}
\bibcite{bernoulli1713ars}{13}
\bibcite{hald1990history}{14}
\bibcite{kolmogorov1933grundbegriffe}{15}
\bibcite{ramsey1926}{16}
\bibcite{pearson1892}{17}
\bibcite{tukey1977}{18}
\bibcite{hintze1997}{19}
\bibcite{anscombe1973graphs}{20}
\bibcite{spearman1904}{21}
\bibcite{bayes1763}{22}
\bibcite{billingsley1995}{23}
\bibcite{definetti1974}{24}
\bibcite{doob1953}{25}
\bibcite{kolmogorov1933}{26}
\bibcite{laplace1812}{27}
\bibcite{lebesgue1902}{28}
\bibcite{vonmises1928}{29}
\bibcite{huygens-games}{30}
\bibcite{bernoulli-ars}{31}
\bibcite{laplace-probability}{32}
\bibcite{poisson-judgements}{33}
\bibcite{cardano-ludo}{34}
\bibcite{gauss-astronomy}{35}
\bibcite{kolmogorov-foundations}{36}
\bibcite{pearson-skewness}{37}
\bibcite{lebesgue1902}{38}
\bibcite{doob1953}{39}
\bibcite{vonmises1928}{40}
\bibcite{bayes1763}{41}
\bibcite{laplace1812}{42}
\bibcite{definetti1974}{43}
\bibcite{billingsley1995}{44}
\bibcite{jaynes2003}{45}
\bibcite{student1908}{46}
\bibcite{fisher1925}{47}
\bibcite{fisher1935}{48}
\bibcite{pearson1900}{49}
\bibcite{welch1947}{50}
\bibcite{yates1934}{51}
\bibcite{greenwood1911}{52}
\bibcite{pascal1654}{53}
\bibcite{hacking1965logic}{54}
\bibcite{bayes1763essay}{55}
\bibcite{laplace1814philosophical}{56}
\bibcite{feller1950probability}{57}
\bibcite{jaynes2003probability}{58}
\bibcite{doob1953stochastic}{59}
\gdef \@abspage@last{99}
