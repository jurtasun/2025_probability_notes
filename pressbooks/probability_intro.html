<p class="p1"><em>The theory of probabilities is at bottom</em></p>
<p class="p1"><em>nothing but common sense reduced to</em></p>
<p class="p1"><em>calculation.</em></p>
<p class="p1">— Pierre-Simon Laplace</p>
&nbsp;
<h3>The purpose of these notes</h3>
<p class="p1" style="text-align: justify;">In the following pages one will find an introductory course to the theory of probability and statistical inference, aiming to cover both foundations and basic mathematical concepts, but also practical tools to deal with real data science problems, such as bayesian probability and hypothesis testing. The text is composed by five chapters, together with some appendix sections reviewing basic mathematical notions, and a bibliographic note. The purpose of these lecture notes is to make both probability and statistical analysis an easy, engaging and exciting topic for anyone interested, without the need for prior experience.</p>
<p style="text-align: justify;">Both, predictive probability and descriptive statistics have deep historical roots, from ancient works on chance and divination to modern scientific topics oriented towards information theory, modelling and data analysis. As one could guess, rivers of ink have been written about such topics, and endless literature sources are available. However, after following many different courses at both bachelor and postgraduate levels and teaching such topics myself during the last three years, I have found that most resources belong, almost certainly, to one of the next three classes. Either (<em>i</em>) deeply mathematical, and hence out of reach for most experimental or clinically oriented scientists, (<em>ii</em>) laboratory oriented, focusing on inference and experimental design, and hence missing most of the mathematical background, or (<em>iii</em>) with a direct focus towards programming and computation, relying on domain specific notebooks (Python, R, Matlab, SPSS, etc), and online resources with precompiled libraries for simulation, which again miss most of the mathematical and formal intuitions. Indeed, the misuse of statistics in experimental sciences is a critical topic in modern times, as mathematicians have extensively discussed during the last decades. The well-known article by John P. A. Ioannidis, <em>"Why most published research findings are false"</em> [...], serves as a prominent example, and it may serve as motivation for a rigorous study.</p>
<p style="text-align: justify;">As a matter of fact, when it comes to modern statistics, data analysis or experimental design, concepts like <em>stochasticity, randomness, sampling, hypothesis, significance, statistic test, p-value—</em>just to mention some of them—are frequently used, but for most bachelor and even master's level degrees they are rarely introduced or properly defined. Indeed, for most experimental and clinically oriented degrees, they are not introduced at all, leaving the student with just a superficial knowledge relying on intuition about some particular cases. Hence, developing high-quality, simple, and accessible open-source material for present and future generations, covering both probability and statistical inference from both a fundamental <em>and</em> applied level, remains an urgent task for scientists and educators.</p>
<p style="text-align: justify;">This is intended to be a complete introductory course, and no previous mathematical background is required. By keeping the theory simple and always followed by examples, we will build the definitions and quantities from simple to more complex. All mathematical formulas will be introduced with rigorous notation, but keeping in mind that it is not formal aspect, but the intuitions and the general understanding, what we are after. Additionally, all topics will be introduced alongside with some short historical discussion and context, as we believe that a purely technical knowledge just grasps the complexity—and beauty—of scientific topics. As one could anticipate already, a proper understanding of ideas such as uncertainty, variation, chance, probability, inference, etc, can be applied to describing a vast amount of real-world phenomena, ranging from gambling and descriptive analysis to modelling in physics, biology, machine learning and quantum mechanics, among many others.</p>
<p style="font-weight: 400;">For further reading, we provide here some example textbooks approaching introduction to probability and statistical inference at different depths. They range from more technical and mathematically, problem-solving oriented, to more humanistic and philosophical:</p>

<ul>
 	<li>A simple, intuitive introduction to statistics with few mathematical concepts is provided in Spiegelhalter's <em>"The Art of Statistics: How to Learn from Data"</em> [...].</li>
 	<li>A more foundational textbook, with more advanced mathematical approach, can be found at DeGroot and Schervish's <em>"Probability and Statistics"</em> [...].</li>
 	<li>For a philosophical and historical perspective on probability and statistics, please find Forster and Bandyopadhyay's handbook <em>"Philosophy of Statistics"</em> [...].</li>
 	<li>A comprehensive introduction with focus on practical applications and modern data analysis tools can be found at Diez, Barr &amp; Mine <em>"OpenIntro Statistics"</em> [...].</li>
 	<li>For fundamental concepts in probability and statistics, including random variables, distributions and statistical inference, with practical examples and exercises follow Hossein Pishro-Nik's <em>"Probability, Statistics &amp; Random Processes"</em> [...].</li>
</ul>
&nbsp;
<h3>The roots of probability</h3>
<p style="text-align: justify;">As one might expect, the origins of probability and related concepts can be traced back to very ancient times. Civilizations such as the Babylonians, Egyptians, and Greeks already encountered uncertainty in various aspects of life, including commerce, games of chance, and divination. Consequently, notions of randomness and stochasticity have deep historical roots. For instance, archaeological findings suggest that the earliest known dice date back over 5,000 years, reflecting humanity’s early fascination with chance and unpredictability [...]. Although these cultures had not yet developed a formal mathematical theory of probability, they recognized recurring patterns in random events and attempted to anticipate outcomes through either empirical observation or superstition. For a detailed historical overview, see Florence Nightingale's 1962 manuscript <em>"Games, Gods and Gambling"</em> [...].</p>
<p style="text-align: justify;">While classical Greek and Roman philosophers frequently discussed the nature of chance, necessity, and determinism, their inquiries remained primarily philosophical rather than mathematical. Thinkers such as Cicero distinguished between events occurring by chance and those determined by fate, foreshadowing later developments in probability theory [...]. These early ideas, though lacking quantitative formalism, provided the intellectual foundation for later scientific inquiry into randomness and causality.</p>
<p style="text-align: justify;">A significant shift occurred during the late medieval and early Renaissance periods, when more rigorous mathematical ideas began to shape. Italian mathematician and gambler Gerolamo Cardano (1501–1576) made substantial contributions to the mathematical analysis of chance. His work <em>"Liber de Ludo Aleae"</em> (<em>"Book on Games of Chance"</em>) [...], posthumously published in 1663, is one of the earliest known texts to explore probability through the analysis of gambling problems. However, Cardano’s reasoning, while insightful, lacked the symbolic clarity and mathematical rigour of modern probability theory. Readers consulting the original manuscript will notice an ambiguous and sometimes inconsistent symbolic system, quite unlike the formal structures we use nowadays.</p>
<p style="text-align: justify;">The formalization of probability as a mathematical discipline did not occur until the 17th century, most notably through the seminal correspondence between Blaise Pascal and Pierre de Fermat. Their work, motivated by problems such as finding a fair division of stakes in interrupted games of chance, introduced foundational concepts such as combinatorics, expected value, and variance \cite{devlin2008unfinished}. These developments paved the way for later contributions by Christiaan Huygens, who in 1657 wrote the first published textbook on probability <em>"De Ratiociniis in Ludo Aleae"</em> (<em>"On Reasoning in Games of Chance"</em>), and Jacob Bernoulli, whose 1713 <em>"Ars Conjectandi"</em> (<em>"The Art of Conjecturing"</em>) remains among the most influential early texts in the field. Their works, alongside with many others, collectively laid the groundwork for the probabilistic and statistical methods that foreshadow modern scientific reasoning [...].</p>
<p style="text-align: justify;">The modern axiomatic formulation of probability was introduced in the early 20th century by the Russian mathematician Andrey Kolmogorov. In his 1933 monograph <em>"Grundbegriffe der Wahrscheinlichkeitsrechnung"</em> (<em>"Foundations of the Theory of Probability"</em>) [...], Kolmogorov synthesized classical and frequentist ideas into a rigorous mathematical framework based on measure theory. His axioms remain the standard foundation for probability theory to this day, as we will see in Chapter 2 of these lecture notes . It may seem surprising that a concept with such ancient origins was not formally axiomatized until such recent times, and we will return to Kolmogorov’s formulation and its implications in greater detail in Chapter 5. Nevertheless, philosophical discussions about the interpretation of probability and its relation to the physical sciences—especially in the context of determinism, epistemology and modern topics such as quantum mechanics—predate Kolmogorov's formulation and continue to evolve to this day [...].</p>
<p style="text-align: justify;"></p>